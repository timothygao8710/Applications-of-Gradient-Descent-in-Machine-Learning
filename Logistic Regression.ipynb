{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07e759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error function: calculuate cross entropy\n",
    "#mean squared error is used for linear regression\n",
    "\n",
    "#weights: relative importances of features\n",
    "#bais: \"shift\"/location of the activation function\n",
    "#activiation function is sigmoid\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate for gradient descent\n",
    "LR = 0.0015\n",
    "\n",
    "#number of random starting points to take\n",
    "n_iters = 100\n",
    "\n",
    "#number of steps to take in GD\n",
    "n_steps = 1000\n",
    "\n",
    "#feature inputs -> dotted weights, + bias -> y = sigmoid(x) -> find_loss(y)\n",
    "\n",
    "#we can write in terms of wegiths and biases (substitute into the find loss function)\n",
    "#feature inputs -> dotted weights, + bias -> y = sigmoid(x) -> find_loss(y) -> \n",
    "\n",
    "weights, biases = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tanh, ReLU (less computation time)\n",
    "#Why choose sigmoid?\n",
    "\n",
    "#Activation function: \n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Computes sigmoid function to calculate probability based on single-value output of logistic regression\n",
    "    and predictions. \n",
    "    Input: Computed prediction value\n",
    "    Returns: scalar probability between (0, 1)\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def cross_entropy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between predicted probabilities and actual 0/1 classifications. \n",
    "    Input: predictions vector of predicted classes' probabilities\n",
    "           targets vector of actual 0/1 classes\n",
    "    Returns: scalar representing loss\n",
    "    \"\"\"\n",
    "    n = predictions.shape[0]\n",
    "    return -(1/n) * np.sum(targets*np.log(predictions) + (1-targets)*np.log(1-predictions))\n",
    "\n",
    "#E'(weights, biases)\n",
    "#Other activation function and their derivatives given here: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "def calc_dir(weights, bias, predicted, actual):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: weights = coefficients for features\n",
    "           bias = single-value added to prediction output     \n",
    "           predicted = predicted 0/1 classes for classification output\n",
    "           actual = actual 0/1 classes for classification output\n",
    "    Returns: tuple of 2 elements\n",
    "             gradient descent vector <dw, db> = vector of greatest descent for weights and biases, respectively\n",
    "    \"\"\"\n",
    "    N = len(weights)\n",
    "    dW = (1/N) * 2 * np.dot(weights.T, (predicted - actual))\n",
    "    dB = (1/N) * 2 * np.sum(predicted - actual)\n",
    "    \n",
    "    return [dW, dB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e532d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(weights, bias, x, y, LR):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to optimize initial starting weight and biases\n",
    "    \n",
    "    Input: x = training set containing input features\n",
    "           y = training set containing expected classification\n",
    "           weights = n-d vector/numpy_array\n",
    "           bias = scalar\n",
    "           LR = scalar, learning rate\n",
    "    Returns: tuple of 2 elements, (weights, bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in n_steps:\n",
    "        #for each row of features, we generate a corresponding number by multiplying each weight by feature value\n",
    "        predicted_vals = np.dot(weights.T, x) + bias\n",
    "        print(predicted_vals.shape)\n",
    "        \n",
    "        #for each feature value, we predict its class probability utilizing the sigmoid function\n",
    "        predicted_probs = sigmoid(predicted_vals)\n",
    "        print(predicted_probs.shape)\n",
    "        \n",
    "        print(\"On step \" + str(i) + \"th step, loss equals \" + cross_entropy(predicted_probs, y))\n",
    "        \n",
    "        #calculated derivatives\n",
    "        derivs = calc_dir(weights, bias, predicted_probs)\n",
    "        dW = derivs[0], dB = derivs[1]\n",
    "        \n",
    "        #\"descent\" down\n",
    "        weights -= LR * dW\n",
    "        bias -= LR * dB\n",
    "        \n",
    "    return [weights, bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aacc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y):\n",
    "    \"\"\"\n",
    "    Optimizes weights and biases given initial training set. Utilizes entire training set (must train-test-split beforehand)\n",
    "    \n",
    "    Input: x = model input of training dataset, 2darray containing m samples (entries) and n features\n",
    "           y = expected model output of training dataset, 1darray containing n classification results (0/1)\n",
    "\n",
    "    \"\"\"\n",
    "    global weights, bias\n",
    "    m, n = x.shape\n",
    "    \n",
    "    #random starting points: vector/np_array of size n\n",
    "    weights = np.random.rand(n)\n",
    "    #random starting points: random bias\n",
    "    bias = np.random.rand(1)[0]\n",
    "    \n",
    "    res = gradient_descent(weights, bias, x, y, LR)\n",
    "    weights = res[0], bias = res[1]\n",
    "    \n",
    "    #Start at a random location\n",
    "#     for _ in n_iters:\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8795d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_prob(features);\n",
    "    \"\"\"\n",
    "    Utilizes pre-optimized weights and biases to outputs class probability given nd-vector of features\n",
    "    \n",
    "    Input: nd-vector of feature\n",
    "    Output: scalar, class probability of features\n",
    "    \"\"\"\n",
    "\n",
    "    predicted_val = np.dot(weights.T, x) + bias\n",
    "    predicted_prob = sigmoid(predicted_vals)\n",
    "    \n",
    "    return predicted_prob\n",
    "\n",
    "def predict_probs(x):\n",
    "    \"\"\"\n",
    "    Utilizes pre-optimized weights and biases to outputs class probability given m entries of nd-vector of features\n",
    "    \n",
    "    Input: m by n matrix of feature\n",
    "    Output: m-d vector of class probabilities for each entry\n",
    "    \"\"\"\n",
    "    return predict_single_prob(x)\n",
    "\n",
    "def predict(x):\n",
    "    \"\"\"\n",
    "    Utilizes pre-optimized weights and biases to outputs class given m entries of nd-vector of features\n",
    "    \n",
    "    Input: m by n matrix of feature\n",
    "    Output: m-d vector of 0/1 class for each entry\n",
    "    \"\"\"\n",
    "    return round(predict_probs(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
