{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc54428c",
   "metadata": {},
   "source": [
    "# Applications of Gradient Descent In Machine Learning\n",
    "by Timothy Gao\n",
    "\n",
    "## Table Of Contents:\n",
    "* [1: Logistic Regression](#1)\n",
    "    * [1.1: Introducing Weights](#1.1)\n",
    "    * [1.2: Activation Function](#1.2)\n",
    "    * [1.3: Introducing Bias](#1.3)\n",
    "    * [1.4: Summary](#1.4)\n",
    "    \n",
    "* [2: Gradient Descent](#2)\n",
    "    * [2.1: Introducing Loss Functions](#2.1)\n",
    "    * [2.2: Minimizing Loss Functions](#2.2)\n",
    "    * [2.3: Introducing Gradient Descent](#2.3)\n",
    "    * [2.4: Learning Rate](#2.4)\n",
    "    * [2.5: Local Minimums and Saddle Points](#2.5)\n",
    "    * [2.6: Deriving and Finding $\\nabla J$](#2.6)\n",
    "\n",
    "* [3: Ideas in Practice (Implementation With Detailed Comments)](#3)\n",
    "* [4: Gradient Descent Visualization](#4)\n",
    "* [5: References](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6d039-3079-451d-b426-697063370234",
   "metadata": {},
   "source": [
    "## Section 1: Logistic Regression <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "#### 1.1: Introducing Weights <a class=\"anchor\" id=\"1.1\"></a>\n",
    "\n",
    "We wish to predict a probability that a cell is cancerous based on a set of input variables, called features (e.g., cell radius, texture, concavity). In machine learning, this is called a binary classification problem: we wish to predict the probability for a binary class (yes or no cancer, 1 or 0).\n",
    "\n",
    "Let each feature be an element of the vector $x = < x_1, x_2, ..., x_n >$\n",
    "\n",
    "Ultimately, we desired a scalar value. How can we convert this n-dimensional vector of features into a single scalar? Dot Product!\n",
    "\n",
    "Let weights be the vector $w = < w_1, w_2, ... w_n >$. Then $s = w \\cdot x = \\Sigma^{n}_{i=1} w_i x_i$. Visually:\n",
    "\n",
    "Utilizing a visually example, we have:\n",
    "\n",
    "<center><img width=\"400\" height=\"400\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/dotproduct.png\"/></center>\n",
    "<sub> note these are made-up numbers for demonstration purposes <\\sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a74431-0fe7-48b1-b985-62bdf72d7f9b",
   "metadata": {},
   "source": [
    "\n",
    "Intuitively, the weight of each feature represents its overall \"importance\" in the prediction outcome. However, $s$ does not denote our predicted probability.\n",
    "\n",
    "#### 1.2: Activation Function <a class=\"anchor\" id=\"1.2\"></a>\n",
    "\n",
    "Now, we need a function $\\sigma (s)$ to transform $s \\in \\mathbb{R}$ into our desired probability, $p \\in (0,1)$\n",
    "\n",
    "These functions are called activation functions in machine learning. Here are some examples:\n",
    "\n",
    "<center><img width=\"500\" height=\"500\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/activationfunctions.png\"></center>\n",
    "\n",
    "One of the most used activation functions in many machine learning models is the sigmoid function:\n",
    "\n",
    "$$ \\sigma (z) = \\frac{1}{-e^{z}}$$\n",
    "\n",
    "The sigmoid function has a peculiar advantage for being easily differntiable, which will prove very helpful later.\n",
    "\n",
    "$$ \\sigma ' (z) = \\frac{\\mathrm{d} \\sigma}{\\mathrm{d} z} = (\\sigma(z))(1 - \\sigma(z))$$\n",
    "\n",
    "<center><img width=\"1000\" height=\"1000\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/sigmoid.png\"></center>\n",
    "\n",
    "#### 1.3: Introducing Biases <a class=\"anchor\" id=\"1.3\"></a>\n",
    "\n",
    "In order to horizontally shift $s$ such that it fits within the \"useful\" domain of the sigmoid function (so it doesn't just evaluate to 0.00001 or 0.99998 every time), we add an additional bias, a scalar $b$. Putting everything together, we have:\n",
    "\n",
    "$$ z = w \\cdot x + b $$\n",
    "\n",
    "#### 1.4: Summary <a class=\"anchor\" id=\"1.4\"></a>\n",
    "\n",
    "$$ p = \\sigma (z) = \\frac{1}{-e^{w \\cdot x + b}} $$\n",
    "\n",
    "In machine learning, we usually denote model parameters, in this case $w$ and $b$, as $\\theta$. More formally,\n",
    "\n",
    "$$ \\theta = <w, b> $$\n",
    "\n",
    "As a function of theta, our model makes predictions on\n",
    "\n",
    "$$ f(x, \\theta) = \\frac{1}{-e^{{\\theta}_w} \\cdot x + {\\theta}_b} $$\n",
    "\n",
    "Now, the problem of predicting probability is reduced to the problem of finding optimal weights ($w$) and bias ($b$). The finding of optimal weights and bias is the \"learning\" part of machine learning. We will focus on a particular algorithm called called Logistic Regression, that makes predictions based on $\\sigma (w \\cdot x + b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca151ff1-f2cb-471b-99db-1730fd3a1ea5",
   "metadata": {},
   "source": [
    "## Section 2: Gradient Descent <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "\n",
    "#### 2.1: Introducting Loss Functions <a class=\"anchor\" id=\"2.1\"></a>\n",
    "\n",
    "In order to find the optimal weights and bias, we first need to define what \"optimal\" means. In machine learning, optimal means minimizing error. This error, depends on the difference between our predicted outcome and the expected outcome, and can be defined as a function called the loss function or cost function. The smaller our loss function, the better our model is doing. In your AP Statistics class you may have learned about least squared error ($\\sum (p - y)^2$), which is indeed a type of loss function, but it's typically used in another type of task in machine learning called regression. For binary classification, there are a few popular options:\n",
    "\n",
    "- Cross Entropy: $L_{CE}(y, p) = -{(y\\log(p) + (1 - y)\\log(1 - p))}$\n",
    "\n",
    "- Negative Loglikelihood: $L_{NL}(y, p) = -{\\log(p(y))}$\n",
    "\n",
    "- Hinge Loss: $L_{HL}(y, p) = max(0, 1 - y \\cdot (\\left \\lceil{p + 0.5}\\right \\rceil))$\n",
    "\n",
    "<sub> Note y is the actual (expected) binary 0/1 classes, while p is our predicted probability $\\in [0, 1]$ <\\sub> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff47ed3-62e5-4c90-9bfe-cd88c8fed50a",
   "metadata": {},
   "source": [
    "Logistic Regression uses Cross Entropy, a standard and utilitarian loss function in binary classification.\n",
    "\n",
    "Recall from the previous section that $p = \\sigma (w \\cdot x + b)$. Plugging this in:\n",
    "    \n",
    "$$ L_{CE}(w, b, y) = {y\\log(\\sigma (w \\cdot x + b))) + (1 - y) \\log (1 - \\sigma (w \\cdot x + b))} $$\n",
    "    \n",
    "Again, $y$ is our actual/expected binary classes, $w$ is the vector of our weights, and $b$ is the scalar of our bias.\n",
    "    \n",
    "To improve our model, we need to minimize this function... This sounds like Multivariable Calculus!\n",
    "\n",
    "#### 2.2: Minimizing the Loss Function <a class=\"anchor\" id=\"2.2\"></a>\n",
    "\n",
    "In order for our model to be able to \"intellegently\" deal with any input we give it, it needs to train on large amounts of diverse data. In general, we want to select weights and biases that will minimize $L_{CE}$ over $m$ sets of observations, each containing $n$ features and $1$ expected output ($y$). For example, here is what UCI's breast cancer dataset:\n",
    "    \n",
    "<center><img width=\"2000\" height=\"2000\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/datasetdemo.png\"></center>\n",
    "\n",
    "We have 31 features ($n = 31$) and a total of 569 observations/rows ($m = 569$).\n",
    "    \n",
    "To make our model more versatile and adaptable, we need to minimize the loss function over all $m$ observations. More formally, we aim to minimize:\n",
    "\n",
    "$$ \\frac{1}{m} \\sum_{i=1}^{m} L_{CE}({\\theta}, y_i) $$\n",
    "\n",
    "After putting everything together, we wish to find:\n",
    "\n",
    "$$ \\text{argmin}_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} {y_i \\log(\\sigma ({\\theta}_w \\cdot x_i + {\\theta}_b)) + (1 - y_i) \\log (\\sigma ({\\theta}_w \\cdot x_i + {\\theta}_b))} $$\n",
    "    \n",
    "This is exactly the task of gradient descent.\n",
    "    \n",
    "#### 2.3: Introducing Gradient Descent <a class=\"anchor\" id=\"2.3\"></a>\n",
    "    \n",
    "In gradient descent, we call this function $J(\\theta) = J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} L_{CE}({\\theta}, y_i)$.\n",
    "    \n",
    "Here are the steps of a bare-bones gradient descent algorithm:\n",
    "1. Generate a random ${\\theta}_t = {\\theta}_0$, which corresponds to the point $({{\\theta}_0}_{w}, {{\\theta}_0}_{b}, J({\\theta}_0))$\n",
    "\n",
    "\n",
    "2. Determine the direction of greatest descent from $({{\\theta}_t}_{w}, {{\\theta}_t}_{b}, J({\\theta}_t))$, which we learned from multivar is in the opposite direction of the gradient vector, or $- \\nabla J({\\theta}_t)$\n",
    "\n",
    "\n",
    "3. Modify $\\theta$ by performing vector addition with $-\\nabla J({\\theta}_t)$ multiplied by a constant, $\\eta$, which is called the learning rate. More formally, ${\\theta}_{t+1} = {\\theta}_t - {\\eta} \\cdot \\nabla J({\\theta}_t)$\n",
    "\n",
    "\n",
    "4. Repeat from step 2 with ${\\theta}_t$ iff ${\\theta}_t \\neq {\\theta}_{t+1}$ and $t < C$, where $C$ is some constant indicating the maximum number of iterations\n",
    "    \n",
    "Here is a visualization of the outlined process, with vectors indicating each iteration of the algorithm:\n",
    "\n",
    "<center><img width=\"700\" height=\"700\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/GDVis.png\"></center>\n",
    "\n",
    "In this example, we took 13 steps from our initial ${\\theta}_0$, so ${\\theta}_{final} = {\\theta}_{13}$.\n",
    "\n",
    "Informally, we can think of this as incremental taking steps down a \"mountain,\" which is the surface curve of our function $g$. Each step, each iteration, we are getting closer and closer to reaching the global minimum.\n",
    "\n",
    "#### 2.4: Learning Rate <a class=\"anchor\" id=\"2.4\"></a>\n",
    "\n",
    "The learning rate, $\\eta$, determines how fast we descend down $J$. Recall the equation for modifying $\\theta$:\n",
    "\n",
    "$$ {\\theta}_{t+1} = {\\theta}_t - {\\eta} \\cdot \\nabla J({\\theta}_t) $$\n",
    "\n",
    "The larger the $\\eta$, the faster we descent. However, this comes at the cost of less accuracy in pinpointing the exact location of the minimum. Here's an oversimplified but helpful visualization in 2D:\n",
    "\n",
    "<center><img width=\"1000\" height=\"1000\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/LRTypes.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1fac9b-a378-4b1b-ac64-5e07963ff0b1",
   "metadata": {},
   "source": [
    "Note how the slow learning rate sacrafises computational efficiency (speed), while the too high learning rate results in inaccurate pinpoint or divergent behavior, where $J({\\theta}_{t+1}) > J({\\theta}_t)$\n",
    "\n",
    "The learning rate for any particular machine learning application undergoes a process called [Hyperparameter Tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization) to find the optimal $\\eta$ (which is a hyperparameter of the model) for its particular application and $g$ function.\n",
    "\n",
    "Additionally, many implementations of gradient descent choose to reduce $\\eta$ overtime along with ${\\theta}$, first quickly descending to reach a general region that is likely to contain the global minimum, then more slowly descending to converge on it (see section 4, this idea is implemented in my simulation).\n",
    "\n",
    "The challenge of finding the optimal learning rate can be tackled by utilizing hyperparameter tuning and changing $\\eta$ overtime, but another significant obstacle in Logisitic Regression is escaping local minimums and saddle points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf340c-3137-4f02-84f7-cc12f13c38ea",
   "metadata": {},
   "source": [
    "#### 2.5: Local Minimums and Saddle Points <a class=\"anchor\" id=\"2.5\"></a>\n",
    "\n",
    "As we've learned in multivar, saddle points and local critical points ($f_x = 0, f_y = 0$) have the same $\\nabla J$ as global minimums. Many variations of the bare-bones, classical gradient descent have sprung up to address this issue. For example:\n",
    "\n",
    "- Stochastic gradient descent: we add noise on top of our loss function, and perform gradient descent on what's effectively the \"noisy gradient\", so as to introduce potential paths to escape out of a local minimum/saddle point.\n",
    "- Momentum gradient descent: we treat $\\theta$ intuitively as a ball that is rolling downhill, adding the idea of momentum so that it can escape local minimums/saddle points as it \"rolls downhill\".\n",
    "\n",
    "There are other variations, and this gif highlights popular ones, using a contour graph:\n",
    "\n",
    "<center><img width=\"500\" height=\"500\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/GDvariations.gif\"></center>\n",
    "\n",
    "The bottom graph demonstrates $J(\\theta)$ over number of iterations ($t$).\n",
    "\n",
    "Another simple idea is to perform gradient descent on many random points to maximize our chance of finding the global minimum on one of them, but this comes at the cost of computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d8f9a-5758-4cb5-83fb-0cb9b6f8ff34",
   "metadata": {},
   "source": [
    "#### 2.6: Deriving and Finding $\\nabla J$ <a class=\"anchor\" id=\"2.6\"></a>\n",
    "\n",
    "First, recall from section 2.2 that\n",
    "\n",
    "$$ J(\\theta) = J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} L_{CE}({\\theta}, y_i) = \\frac{1}{m} \\sum_{i=1}^{m} {y_i \\log(\\sigma ({\\theta}_w \\cdot x_i + {\\theta}_b)) + (1 - y_i) \\log (\\sigma ({\\theta}_w \\cdot x_i + {\\theta}_b))} $$\n",
    "\n",
    "\n",
    "Second, recall from section 1.2 the derivative of a sigmoid:\n",
    "\n",
    "$$ \\sigma ' (z) = \\frac{\\mathrm{d} \\sigma}{\\mathrm{d} z} = (\\sigma(z))(1 - \\sigma(z))$$\n",
    "\n",
    "Because both the loss function (cross entropy) and activation function (sigmoid) are differentiable, we find that \n",
    "\n",
    "<center><img width=\"350\" height=\"350\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/deriv.png\"></center>\n",
    "\n",
    "Note that y-hat, or predicted output, is equivalent to $p$, the predicted probability. Here, $N$ denotes the number of observations/samples/rows in our training dataset instead of $m$.\n",
    "\n",
    "Let's start by taking the partial derivative of the loss function with respect to a single weight $w_j$:\n",
    "\n",
    "<center><img width=\"500\" height=\"500\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/derivAA.png\"/></center>\n",
    "\n",
    "Then, using the chain rule and the fact that $\\frac{d}{dx} ln(x) = \\frac{1}{x}$:\n",
    "\n",
    "<center><img width=\"600\" height=\"600\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/derivA.png\"/></center>\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "<center><img width=\"500\" height=\"500\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/derivB.png\"/></center>\n",
    "\n",
    "Now plugging in derivative of a sigmoid and using the chain rule again:\n",
    "\n",
    "<center><img width=\"700\" height=\"700\" src=\"https://raw.githubusercontent.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/main/images/derivC.png\"/></center>\n",
    "\n",
    "For biases, we follow a very similar procedure, but take the partial derivative of $b$ in respect to $J$ instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09978e6c-ac9f-4806-a774-2f5c9ab868a4",
   "metadata": {},
   "source": [
    "## Section 3: Ideas in Practice (Implementation With Detailed Comments) <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "We also test our implementation on a real dataset provided by UC Irvine, predicting breast cancer based on a set of cell parameters. More details on the dataset [here](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)). The dataset is avaliable for download [here](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data/download?datasetVersionNumber=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44975011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  numpy for fast math operations, like dot product\n",
    "import numpy as np\n",
    "\n",
    "# sklearn for breast cancer dataset, a function to split the dataset, confusion matrix, accuracy score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix\n",
    "\n",
    "# pandas for handling breast cancer dataset as a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# seaborn and matplotlib for graphing\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a89bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate for gradient descent\n",
    "LR = 0.0015\n",
    "\n",
    "# number of random starting points to take. We are utilizing the idea in 2.5, taking many random starting points\n",
    "n_iters = 25\n",
    "\n",
    "# number of steps to take in GD\n",
    "n_steps = 1000\n",
    "\n",
    "# weights: relative importances of features\n",
    "# bias: \"shift\"/location of the activation function\n",
    "# activation function is sigmoid\n",
    "weights, biases = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b4ffdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Computes sigmoid function to calculate probability based on w * x + b\n",
    "    Input: Scalar, w * x + b\n",
    "    Returns: Scalar, probability in (0, 1)\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "# loss function:\n",
    "def cross_entropy(predictions, targets, epsilon = 1e-14):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between predicted probabilities and actual 0/1 classifications. \n",
    "    Input: predictions vector of predicted classes' probabilities\n",
    "           targets vector of actual 0/1 classes\n",
    "    Returns: scalar representing loss\n",
    "    \"\"\"\n",
    "    n = predictions.shape[0]\n",
    "    return -(1/n) * np.sum(targets*np.log(predictions + epsilon) + (1-targets)*np.log(1 - predictions + epsilon))\n",
    "\n",
    "# calculating the derivative of J\n",
    "def calc_dir(x, bias, predicted, actual):\n",
    "    \"\"\"\n",
    "    Computes the gradient descent vector or maximum direction of descent for use to update theta\n",
    "    Input: weights = coefficients for features\n",
    "           bias = single-value added to prediction output     \n",
    "           predicted = predicted 0/1 classes for classification output\n",
    "           actual = actual 0/1 classes for classification output\n",
    "    Returns: tuple of 2 elements\n",
    "             gradient descent vector <dw, db> = vector of greatest descent for weights and biases, respectively\n",
    "    \"\"\"\n",
    "    N = len(weights)\n",
    "    \n",
    "    # compute partial derivative for weights\n",
    "    dW = (1/N) * 2 * np.dot(x.T, predicted - actual)\n",
    "    # compute partial derivative for bias\n",
    "    dB = (1/N) * 2 * np.sum(predicted - actual)\n",
    "\n",
    "    return [dW, dB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fad57fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent algorithm, as discussed in section 2\n",
    "def gradient_descent(weights, bias, x, y, LR, report=False):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to optimize initial starting weights and bias from a given starting point\n",
    "    \n",
    "    Input: x = training set containing input features\n",
    "           y = training set containing expected classification\n",
    "           weights = n-d vector/numpy_array\n",
    "           bias = scalar\n",
    "           LR = scalar, learning rate\n",
    "           report = wether prin\n",
    "    Returns: tuple of 2 elements, (weights, bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        # for each row of features, we generate a corresponding number by multiplying each weight by feature value\n",
    "        predicted_vals = np.dot(x, weights.T) + bias\n",
    "        \n",
    "        # for each feature value, we predict its class probability utilizing the sigmoid function\n",
    "        predicted_probs = sigmoid(predicted_vals)\n",
    "\n",
    "        if report:\n",
    "            print(\"On step \" + str(i) + \"th step, loss equals \" + str(cross_entropy(predicted_probs, y)))\n",
    "        \n",
    "        # calculated derivatives\n",
    "        derivs = calc_dir(x, bias, predicted_probs, y)\n",
    "        dW = derivs[0]\n",
    "        dB = derivs[1]\n",
    "        \n",
    "        # \"descent\" down using learning rate\n",
    "        weights -= LR * dW\n",
    "        bias -= LR * dB\n",
    "        \n",
    "    return [weights, bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8a3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function that puts everything together\n",
    "def train(x, y, report = False):\n",
    "    \"\"\"\n",
    "    Optimizes weights and biases given initial training set. Utilizes entire training set (must train-test-split beforehand)\n",
    "    \n",
    "    Input: x = model input of training dataset, 2darray containing m samples (entries) and n features\n",
    "           y = expected model output of training dataset, 1darray containing n classification results (0/1)\n",
    "\n",
    "    \"\"\"\n",
    "    # declare variables to use\n",
    "    global weights, bias\n",
    "    m, n = x.shape\n",
    "    \n",
    "    # we take n_iters random starting points\n",
    "    for _ in range(n_iters):\n",
    "        # random starting points: random vector/np_array of size n\n",
    "        weights = np.random.rand(n)\n",
    "        \n",
    "        # random starting points: random bias\n",
    "        bias = np.random.rand(1)[0]\n",
    "    \n",
    "        # gradient descent from this random starting point\n",
    "        res = gradient_descent(weights, bias, x, y, LR, report)\n",
    "        \n",
    "        # extract content from result\n",
    "        weights = res[0]\n",
    "        bias = res[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a32efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts probability of a given test set (that is completely different from the training set), utilizing weights, bias, and the sigmoid function\n",
    "def predict_probs(x):\n",
    "    \"\"\"\n",
    "    Utilizes pre-optimized weights and biases to outputs class probability given m entries of nd-vector of features\n",
    "    \n",
    "    Input: m by n matrix of feature\n",
    "    Output: m-d vector of class probabilities for each entry\n",
    "    \"\"\"\n",
    "    \n",
    "    # utilize formula discussed in section 1 to acquire values: w * x + b\n",
    "    predicted_vals = np.dot(x, weights.T) + bias\n",
    "    # apply the sigmoid function to convert value to probability\n",
    "    predicted_probs = sigmoid(predicted_vals)\n",
    "    \n",
    "    return predicted_probs\n",
    "\n",
    "# rounds predicted probabilities to the nearest integer (0/1) to predict actual binary classes\n",
    "def predict(x):\n",
    "    \"\"\"\n",
    "    Utilizes pre-optimized weights and biases to outputs class given m entries of nd-vector of features\n",
    "    \n",
    "    Input: m by n matrix of feature\n",
    "    Output: m-d vector of 0/1 class for each entry\n",
    "    \"\"\"\n",
    "\n",
    "    # call predict_probs function to predict probabilities\n",
    "    x = predict_probs(x)\n",
    "    # round the numbers\n",
    "    x = [0 if i < 0.5 else 1 for i in x]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ac56d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-11c5aba455d1>:9: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0/(1.0 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for training Logistic Regression Model:\n",
      "CPU times: user 2.6 s, sys: 6.02 s, total: 8.61 s\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ^ the above displays time for this cell to run\n",
    "\n",
    "# load dataset\n",
    "data = load_breast_cancer()\n",
    "# make variables x, the features, and y, the output class\n",
    "x, y = data.data, data.target\n",
    "# perform train test split with 80% to 20%, dividing our original dataset. The training set will be used to optimize weights and biases, then the completely untouched test set will be used to test its accuracy\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=69)\n",
    "# call the train function to optimize weights and biases based on the training dataset\n",
    "train(x_train, y_train)\n",
    "\n",
    "print(\"Time for training Logistic Regression Model:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4fa04ec-5cc7-4823-9f09-e38312ac1e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final accuracy of the model is 91.22807017543859%.\n",
      "Time for testing Logistic Regression Model:\n",
      "CPU times: user 721 µs, sys: 2.35 ms, total: 3.07 ms\n",
      "Wall time: 577 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-11c5aba455d1>:9: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0/(1.0 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ^ the above displays time for this cell to run\n",
    "\n",
    "# call the predict function on testing dataset\n",
    "y_pred = predict(x_test)\n",
    "\n",
    "print(\"The final accuracy of the model is \" + str(100 * accuracy_score(y_test, y_pred)) + \"%.\")\n",
    "print(\"Time for testing Logistic Regression Model:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5669093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a confusion matrix to better understand our results\n",
    "array = confusion_matrix(y_test, y_pred)\n",
    "# turn the confusion matrix into a DataFrame to plot it\n",
    "df_cm = pd.DataFrame(array, index = ['True Yes', 'True No'],\n",
    "                  columns = ['Predicted Yes', 'Predicted No'])\n",
    "\n",
    "# use matplotlib and seaborn to plot it\n",
    "plt.figure()\n",
    "# use matplotlib and seaborn to plot it\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a039148-36fd-46cc-8faa-98cb673752e4",
   "metadata": {},
   "source": [
    "## Section 4: Gradient Descent Visualization <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "[Link to website (might take some time to load) ](https://gradientdescent.herokuapp.com/) |\n",
    "[Link to code](https://github.com/timothygao8710/Applications-of-Gradient-Descent-in-Machine-Learning/blob/main/webvis.py)\n",
    "\n",
    "As part of my project, I also made a visualization of the gradient descent algorithm on an interactive 3D graph. The user can customize the graph by entering their own equation for the function $f(x, y)$, the maximum and minimum values of the function, as well as the learning rate $\\eta$. After graphing, a visual similar to that in section 2.3 will be displayed, where vectors scaled to the magnitude of each step taken in gradient descent will be shown on the 3D graph. The user can use their cursor to rotate the graph, zoom in and out (by scrolling), pan the graph, take a screenshot, and play around with different camera angles. The minimum value found by gradient descent by also be displayed at the bottom.\n",
    "\n",
    "In my simulation, I employed in the idea in section 2.4, utilizing\n",
    "\n",
    "$$ {\\eta}_{t+1} = {\\eta}_{t} \\cdot \\frac{1}{e^{a}} $$ \n",
    "\n",
    "to reduce the learning rate overtime. $a$ is a constant that I experimentally found to be optimal at around ~$0.3$. This exact formula is inspired by [Simulated Annealing](https://en.wikipedia.org/wiki/Simulated_annealing), a meta-heuristic for optimization problems whose core idea is quite similar to gradient descent.\n",
    "\n",
    "To find the global minimum, I also generated many random points to maximize the chance of finding a global minimum on one of them, as in section 2.5.\n",
    "\n",
    "To actually build the graph itself, I utilized the plotly graphing library, computing many disjoint points on a 2D meshgrid, then connecting them to form the 3D graph. The core idea is that of [Riemann Sums](https://en.wikipedia.org/wiki/Riemann_sum), but extended to three dimensions, and rather than finding the area, we're connecting points to build a visualization. As a consequence of this method of generating the graph, 1) the global minimum found may be off by a tiny amount (as the actual minimum can only be achieved iff it is one of the points computed) and 2) the visualization struggles with graphs with a large domain because it'll have to compute quadratically more points, so the max and min are capped (note however that the gradient descent algorithm could perfectly handle a larger domain, as it does for real machine learning loss function such as the one it faces in this notebook's breast cancer dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bc90b-2eee-4e5b-816e-e94d3203d5c3",
   "metadata": {},
   "source": [
    "## Section 5: References <a class=\"anchor\" id=\"5\"></a>\n",
    "    \n",
    "Fernandes, Antônio Alves Tôrres, et al. \"Read this paper if you want to learn logistic regression.\" Revista de sociologia e politica 28 (2021).\n",
    "    \n",
    "Sanderson, Grant. “Gradient Descent, How Neural Networks Learn | Chapter 2, Deep Learning.” YouTube, YouTube, 16 Oct. 2017, https://www.youtube.com/watch?v=IHZwWFHWa-w. \n",
    "\n",
    "Sebastian Ruder. “An Overview of Gradient Descent Optimization Algorithms.” Sebastian Ruder, Sebastian Ruder, 20 Mar. 2020, https://ruder.io/optimizing-gradient-descent/.\n",
    "\n",
    "Cloud Education, IBM. “What Is Gradient Descent?” IBM, https://www.ibm.com/cloud/learn/gradient-descent. \n",
    "\n",
    "Chapter Logistic Regression - Stanford University. https://www.web.stanford.edu/~jurafsky/slp3/5.pdf. \n",
    "\n",
    "Kumar, Satyam. “Overview of Various Optimizers in Neural Networks.” Medium, Towards Data Science, 9 June 2020, https://towardsdatascience.com/overview-of-various-optimizers-in-neural-networks-17c1be2df6d5.\n",
    "\n",
    "Soni, Devin. “Improving Vanilla Gradient Descent.” Medium, Towards Data Science, 16 July 2019, https://towardsdatascience.com/improving-vanilla-gradient-descent-f9d91031ab1d. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
